<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="jdljun">
    
    <title>
        
            ChatGPT原理笔记 |
        
        Life Records
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/weblogo.jpg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"jdljun.github.io","root":"/","language":"zh-CN","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":false},"style":{"primary_color":"#0066CC","avatar":"/images/weblogo.jpg","favicon":"/images/weblogo.jpg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"抱着草长马发情的伟大真诚去做一切事"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"trigger":"auto","unescape":false,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.3"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Life Records
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">ChatGPT原理笔记</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/weblogo.jpg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">jdljun</span>
                        
                            <span class="author-label">Lv4</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;2023-05-18 00:23:01
    </span>
    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>4.8k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>17 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h3 id="1-ChatGPT与NLP"><a href="#1-ChatGPT与NLP" class="headerlink" title="1 ChatGPT与NLP"></a>1 ChatGPT与NLP</h3><p>ChatGPT（Chat Generative Pre-training Transformer）属于自然语言处理（Natural Language Processing）领域中的对话系统</p>
<h4 id="ChatGPT的建模方式"><a href="#ChatGPT的建模方式" class="headerlink" title="ChatGPT的建模方式"></a>ChatGPT的建模方式</h4><p>生成式（Generative ）：</p>
<ol>
<li><p>输入1 -》 ChatGPT -》输出1</p>
</li>
<li><p>输入1、输出1，输入2 -》 ChatGPT -》 输出2</p>
</li>
<li><p>。。。</p>
</li>
</ol>
<p>一次生成一个字</p>
<h4 id="NLP的发展历史"><a href="#NLP的发展历史" class="headerlink" title="NLP的发展历史"></a>NLP的发展历史</h4><p><strong>1.基于规则的NLP</strong></p>
<p>基于规则的 NLP，是指使用人工编写的规则来处理自然语言</p>
<p>输入：I love you</p>
<p>规则： 定义一个单词映射的规则：I -》 you， you -》I</p>
<p>输出：you love I</p>
<p>缺点：人工</p>
<p><strong>2.基于统计的NLP</strong></p>
<p>基于统计的 NLP 则是利用机器学习算法从大量的语料库中学习自然语言的规律特征</p>
<p>基本特点：</p>
<blockquote>
<p>标注数据 =&gt; 建立模型、确定输入输出 =&gt; 训练模型 =&gt; 利用已训练好的模型进行工作</p>
</blockquote>
<p>例子：对样本数据进行计算回归方程</p>
<p>核心：模型参数</p>
<p>ChatGPT 采用预训练（ Pre-training <strong>）</strong> 技术来完成基于统计的 NLP 模型学习</p>
<p>缺点：黑盒，规则隐性，体现在参数中</p>
<p><strong>3.基于强化学习的NLP</strong></p>
<p>ChatGPT 模型是基于统计的，然而它又利用了新的方法，带人工反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF）</p>
<p>理解：反射机制，模型产生预期的行为，给予正反馈，非预期行为，给予负反馈</p>
<h4 id="NLP技术的发展脉络"><a href="#NLP技术的发展脉络" class="headerlink" title="NLP技术的发展脉络"></a>NLP技术的发展脉络</h4><p>基于规则、统计、强化学习，更是一种思想，往往在同一模型中都有体现</p>
<blockquote>
<p>基于规则：家长手把手教孩子怎么解一道题</p>
<p>基于统计：教解题方法</p>
<p>基于强化学习：只定目标，考到90分就行，孩子怎么做到的，不管 </p>
</blockquote>
<h4 id="ChatGPT的神经网络结构-Transformer"><a href="#ChatGPT的神经网络结构-Transformer" class="headerlink" title="ChatGPT的神经网络结构 Transformer"></a>ChatGPT的神经网络结构 Transformer</h4><p>文字（输入） —&gt;  Transformer –&gt; 文字（输出）</p>
<p>Transformer 的核心是<strong>自注意力机制</strong>（Self-Attention），处理输入的文字序列时，每一个字符以及它们的位置都是一个向量，当关注某一个位置的字符时，可以通过向量计算的方式，得到当前字符与其他字符的关系，而且计算可以并行完成</p>
<h3 id="2-从GPT1-0到ChatGPT"><a href="#2-从GPT1-0到ChatGPT" class="headerlink" title="2 从GPT1.0到ChatGPT"></a>2 从GPT1.0到ChatGPT</h3><p>整体发展脉络：朝着拟人化的方向在发展</p>
<h4 id="GPT初代"><a href="#GPT初代" class="headerlink" title="GPT初代"></a>GPT初代</h4><p>GPT和Bert、ELMO等模型，将NLP带入了大规模神经网络语言模型（Large Language Model, LLM）时代</p>
<p><strong>GPT的语言建模</strong></p>
<p>初代GPT能做什么？</p>
<blockquote>
<p>补全：特朗普是中国人民的好__ __ ？</p>
</blockquote>
<p>可以是：朋友、 老师等等</p>
<p>GPT 初代所做的事情：将一个句子分为两个部分，分析句子的上下文，基于上半部分，补全下半部分</p>
<p><strong>语言的编解码</strong></p>
<p>文字输入 —&gt; ChatGPT –&gt; 文字输出</p>
<p>输入需要处理成模型所能理解的语言，输出需要从模型的语言转化为人类能够理解的原研</p>
<p>输入 —&gt;（Encode）—&gt;  ChatGPT – &gt; (Decode) –&gt; 输出</p>
<h4 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h4><p>从 “根据上文补全下文” —&gt; 多任务学习者（开始走向泛化，一脑多用）</p>
<h4 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h4><p><strong>大模型中的大模型</strong></p>
<p>GPT-3 里的大模型计算量是 GPT-1 的上千倍</p>
<p>训练数据足够大，模型的参数量才够大，才有可能足够智能</p>
<blockquote>
<p>参数量就好比人脑中的神经元，神经元数量足够多，才有可能具有更高的智能</p>
</blockquote>
<p><strong>对话模式</strong></p>
<p>GPT 系列模型都是采用 decoder 解码器结构进行训练的，也就是更加适合文本生成的形式。即输入一句话，输出也是一句话。这被称之为对话模式</p>
<p><strong>小样本学习（Few-Shots）</strong></p>
<p>人工标注样子数据成本极高</p>
<p>GPT3 就提出了小样本学习的概念，简单来讲，就是让模型学习语言时，不需要那么多的样例数据。</p>
<h4 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h4><p>ChatGPT 模型结构上和之前的几代都没有太大变化，主要变化的是训练策略变了。</p>
<p><strong>强化学习</strong></p>
<p>环境模型（Reward 模型）：</p>
<ol>
<li> ChatGPT 会针对某一个问题，生成一个回答</li>
<li>环境模型会对 ChatGPT 生成的答案做评价，评价一个分值出来（如 10 分、3 分等等，高分代表奖励，低分代表惩罚），而不具体给出标准答案</li>
<li>ChatGPT 接收到评价反馈后，可以根据这个数值做模型的进一步训练，朝着生成更加恰当答案的方向拟合。</li>
</ol>
<h3 id="3-ChatGPT是一个语言模型"><a href="#3-ChatGPT是一个语言模型" class="headerlink" title="3 ChatGPT是一个语言模型"></a>3 ChatGPT是一个语言模型</h3><p>语言模型：由计算机来实现类似于人的语言交流、对话、叙述能力，它集中体现在模型能够依赖上下文进行正确的文字输出</p>
<h4 id="ChatGPT语言模型的数学建模"><a href="#ChatGPT语言模型的数学建模" class="headerlink" title="ChatGPT语言模型的数学建模"></a>ChatGPT语言模型的数学建模</h4><p><strong>语言模型的基础建模</strong></p>
<p>语言模型的建模公式可以表示为：</p>
<p>P(w1, w2,…. , wi) = P(w1) * P(w2 | w1) *  P(w3 | w1,w2)  * … * P(wi | w1,w2… w(i-1))</p>
<p>P(wi)表示字符wi在语料库中出现的概率，P(wi | w1,w2… w(i-1))表示在已知前面字符的情况下，字符 wi 出现的概率，即依赖上文，对下文的预测。把每一个字符都按照此方式预测出来，其结果就是整条语句的出现概率。</p>
<blockquote>
<p>补全：特朗普是中国人民的好__ __ ？</p>
</blockquote>
<p>如何补全上面这句话？</p>
<p>方案1： 假设汉字一共有一万个，其中 一、朋、老 这三个汉字在训练材料中出现的概率分别为0.0005，0.0001，0.00002，当随机在好后面填入一个字符时，一的概率最大，老概率次之，朋概率最小</p>
<p>问题：只考虑了汉字出现的概率，没有考虑上下文</p>
<p>方案2：依据上下文的条件概率公式</p>
<p>P（朋|特朗普是中国人民的好） = P（特朗普是中国人民的好朋）/  P（特朗普是中国人民的好） = 假设等于0.5</p>
<p>P（友|特朗普是中国人民的好朋） = P（特朗普是中国人民的好朋友）/  P（特朗普是中国人民的好朋） = 假设等于0.8</p>
<p><strong>N-gram的语言建模</strong></p>
<p>过长的文本会导致条件概率太过复杂的问题，所以一般只考虑某个位置附近的上下文</p>
<p>N-gram：就近考虑距离第i个字符，N个距离以内的字符</p>
<p>则P(wi | w1,w2… w(i-1)) 简化为 P(wi | w(i-n+1), w(i-n+2),… w(i-1)) </p>
<p><strong>语言log化</strong></p>
<p>概率都是小于等于1的，P(w1, w2,…. , wi) = P(w1) * P(w2 | w1) *  P(w3 | w1,w2)  * … * P(wi | w1,w2… w(i-1)) 的连乘会导致乘积越小，最终小数位越来越多，计算机无法处理太多小数位</p>
<p>所以要对上面的公式做log运算</p>
<p>log（P(w1, w2,…. , wi)） = ∑(i)log(P(wi | w1,w2… w(i-1)))</p>
<p>考虑到N-gram</p>
<p>简化为：log（P(w1, w2,…. , wi)） = ∑(i)log(P(wi | w(i-n+1), w(i-n+2),… w(i-1)))</p>
<h4 id="ChatGPT的语言模型"><a href="#ChatGPT的语言模型" class="headerlink" title="ChatGPT的语言模型"></a>ChatGPT的语言模型</h4><p><strong>ChatGPT的建模公式</strong></p>
<p>ChatGPT的建模公式本质上还是log（P(w1, w2,…. , wi)） = ∑(i)log(P(wi | w(i-n+1), w(i-n+2),… w(i-1)))</p>
<p>w1, w2,…. , wi定义U</p>
<p>考虑到模型的大量参数集合Θ</p>
<p>则公式变为 log（P(U） = ∑(i)log(P(wi | w(i-n+1), w(i-n+2),… w(i-1))，Θ )</p>
<p><strong>语言模型中的最大似然概率</strong></p>
<p>log（P(w1, w2,…. , wi)）就是一种最大似然的建模方式。</p>
<p>最大似然估计：通过有代表性的样本去估计总体</p>
<p>总体：人类全部的文字集合</p>
<p>样本：GPT的训练样本</p>
<p><strong>ChatGPT语言模型的训练方法</strong></p>
<p>目标：当能够根据上文，来对下文进行预测</p>
<p>输入：特朗普是中国人民的好</p>
<p>输出：P(朋)=0.5，P(老)=0.3，P(总)=0.2</p>
<p>根据最大似然估计原则，在好后面最大概率的文字是 ”朋“</p>
<p>重复这一过程，最终得到输出：特朗普是中国人民的好朋友</p>
<p>其本质就是在让模型学会文字接龙的游戏，以最大似然的准则来约束模型</p>
<h3 id="4-ChatGPT如何处理文字输入"><a href="#4-ChatGPT如何处理文字输入" class="headerlink" title="4 ChatGPT如何处理文字输入"></a>4 ChatGPT如何处理文字输入</h3><p>  文字 –&gt; (BPE) –&gt; token –&gt; （embedding） –&gt; tensor –&gt; ChatGPT –&gt; tensor –&gt; （选词） –&gt; token –&gt; (逆BPE) –&gt; 输出</p>
<h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><p>Tokenizer ： 文本 –&gt; token –&gt; 文本 的过程</p>
<p><strong>BPE（Byte Pair Encoding）算法</strong></p>
<p>BPE 算法是根据一份 token 词表（Vocabulary），将输入的文本拆解成若干个 token</p>
<p>（基于字符）</p>
<p><strong>Byte-level BPE 算法</strong></p>
<p>字符–&gt;Unicode编码</p>
<p>（基于字节）</p>
<p>Unicode编码数量，基于字符形式构造词表，Vocabulary会非常庞大。而Byte-level BPE 算法 直接操作字节，字节数一共才256，这样能够使得Vocabulary大大减少，但同时也会时候sequence 变得庞大</p>
<p><strong>BPE词表如何训练得到</strong></p>
<p>根据训练文本语料统计</p>
<p>贪心算法：把最常见的一对相邻数据单替换为该数据中没有出现过的一个新单位，反复迭代直到满足停止条件</p>
<p><strong>Tokenizer的好处</strong></p>
<p>克服长尾效应：BPE词表的生成算法决定了，高频词高概率在词表中，低频词更低概率在词表中</p>
<p>多语言支持：基于Unicode或基于Unicode字节</p>
<h4 id="词嵌入-Embedding"><a href="#词嵌入-Embedding" class="headerlink" title="词嵌入(Embedding)"></a>词嵌入(Embedding)</h4><p>token –&gt; tensor(张量)这个过程叫做embedding</p>
<p>tensor(张量)即 多维数组 ，如 a= [ [1.23, 1.231,2.31], [554.44 ,45.2,54 .3] ] 就是一个（2，3）的张量</p>
<p>假设 token 词表（Vocabulary）的数量总共为 N，每一个 token 都用一个 M 维的浮点数张量表示，其中每一个 token 都对应了一行张量，即该 token 的 embedding 表示。</p>
<p>#token→ [0.103,0.034,0.129,−0.219,−0.156,…,0.0284,−0.172]</p>
<p>则所有的词表组成了一个N* × <em>M</em>维度的张量 UW(e)</p>
<p>一个序列中文字的位置信息也很重要，所以需要一张表示位置信息的张量（Position Embedding）,W(p)</p>
<p>ChatGPT具备多轮对话能力，需要将每轮对话需要分割开，，每一轮对话都是一个segment，每一个segment有多个token，这些token具有相同的segment embedding</p>
<p>所以，embedding(Input) = embedding(token)  + embedding(position)  +embedding(segment) </p>
<p><strong>embedding的好处</strong></p>
<p>1.embedding 将文字对应的 token 转换为抽象的固定维度的张量，可进行矩阵运算的那种，基于数学运算的大规模神经网络成为了可能</p>
<p>2.Embedding 建立了自然语言的语义与数学之间的关联关系：两个向量余弦距离越小，向量表示的文字语义越近</p>
<h3 id="5-Attention注意力机制"><a href="#5-Attention注意力机制" class="headerlink" title="5 Attention注意力机制"></a>5 Attention注意力机制</h3><p> 注意力机制：从输入的信息中，重点关注重要信息，忽略不重要的信息</p>
<h4 id="注意力机制的建模"><a href="#注意力机制的建模" class="headerlink" title="注意力机制的建模"></a>注意力机制的建模</h4><p><strong>建立权重模式</strong></p>
<p>若想要模型更加关注到某个字，则可给这个字更高的权重</p>
<p>假设ei表示第i个token的embedding，wi是第i个token的权重，则可对所有token的embedding做一个加权：</p>
<p>h = ∑(i) wi * ei</p>
<p><strong>softmax函数</strong></p>
<p>wi是一个标量，假设经过计算后得到 w = (w1,w2,…,wn) = (1, -2.2, …, -0.1) </p>
<p>对w向总和为1的概率值进行转换</p>
<p>αi = exp(wi)/∑(i)exp(wi)</p>
<p>模型的所有 embedding 加权后的权重</p>
<p>h= ∑(i) αi * ei</p>
<p><strong>自注意力机制</strong></p>
<p>如何计算w：自注意力机制（Self-Attention）</p>
<p>假设对第i个token，有两个向量qi、ki，其权重wi=qi*ki，</p>
<p>最简单的方式：当前token的embedding为q，把其他token的embedding当作k，二者相乘 —— 相当于是在比较当前token和其他token的关联关系</p>
<p>相乘的结果做归一化（除以维度dp的根号），再执行softmax操作</p>
<p>Attention(Q，K，V) = softmax(Q*K/根号dq)*V</p>
<p>实际情况：Q<em>、</em>K<em>、</em>V* 三个模型矩阵都是由 token embedding 矩阵做一个矩阵变换得到的。</p>
<p>Q = Wq * ei， K = Wk * ei， V = Wv * ei， </p>
<p>Wq，Wk，Wv神经网络模型扩展的可训练参数</p>
<h4 id="注意力机制的好处"><a href="#注意力机制的好处" class="headerlink" title="注意力机制的好处"></a>注意力机制的好处</h4><p>解决长文本依赖：注意力机制能够跨越很多个token，找到目标token和关联token</p>
<p>支持并行计算</p>
<h3 id="6-transformer结构"><a href="#6-transformer结构" class="headerlink" title="6 transformer结构"></a>6 transformer结构</h3><p><img src="https://cdn.jsdelivr.net/gh/jdljun/imghosting/img/transformer.png"></p>
<h4 id="transFormer结构"><a href="#transFormer结构" class="headerlink" title="transFormer结构"></a>transFormer结构</h4><p><strong>稀疏 Transformer</strong></p>
<p>Sparse Transformer 的核心是 Sparse Self-Attention （稀疏自注意力机制）</p>
<p>： 只选择部分token计算权重</p>
<p>可以有效减少计算</p>
<p><strong>稀疏 Transformer 原理</strong></p>
<p><em>S</em>14=(0,1,2,3,5,7,10,13) 表示 计算第14个位置的token时，只计算括号内的索引位置的token</p>
<p>则K和V的运算相应的也只计算索引位置</p>
<p><strong>多头（multi-head）注意力机制</strong></p>
<p>稀疏 Transformer存在某些token信息丢失的问题，可以通过多头注意力机制，多计算几次，综合多次计算的结果</p>
<p><strong>Normalization 正规化</strong></p>
<p>x-μ/标准差</p>
<p><strong>Dropout 机制</strong></p>
<p>假设 Attention 层输出了一组 token 矩阵，模型经过一个完全随机的 dropout 之后，其结果中的某些元素就被置为 0，防止过拟合（Overfitting）</p>
<p><strong>ResNet 残差模块</strong></p>
<p>Attention 模块的输入 embed 和输出结果有一个叠加，这种叠加操作被称为残差模块。之所以这么操作，主要是为了方便模型的训练过程中，梯度不会消失或爆炸</p>
<p><strong>Linear Feed-forward 全连接层</strong></p>
<p>为模型增加参数，增强模型的拟合能力</p>
<h3 id="7-Encoder-Decoder"><a href="#7-Encoder-Decoder" class="headerlink" title="7 Encoder-Decoder"></a>7 Encoder-Decoder</h3><h4 id="GPT-中的编解码架构"><a href="#GPT-中的编解码架构" class="headerlink" title="GPT 中的编解码架构"></a>GPT 中的编解码架构</h4><p>文本 –&gt; token </p>
<p>token embedding   + position embedding </p>
<p>N 层transformer</p>
<p>最后产出一个K维向量</p>
<h4 id="解码得到输出-Token"><a href="#解码得到输出-Token" class="headerlink" title="解码得到输出 Token"></a>解码得到输出 Token</h4><p><strong>贪婪搜索 Greedy Search</strong></p>
<p>哪个token概率最大，就输出哪个token，即按照最大概率值选择模型输出的 token</p>
<p>缺陷：漏掉一些概率值稍低的正确答案</p>
<p><strong>束搜索 Beam Search</strong></p>
<p>选择最高概率的若干选项，再在其中随机抽取的方式叫做束搜索</p>
<p>可以人为设置一个数量K，从top K 概率的token中抽取</p>
<p><strong>核搜索 Nucleus Search</strong></p>
<p>前 T 个概率值加起来大于 top_p（介于 0~1 之间的值），则以这 T 个值作为最终的抽取范围</p>
<p>top_p越大，则供抽取的token越多</p>
<p><strong>温控搜索 Temperature Search</strong></p>
<p>保证各个 token 的概率值相对大小排序不变的情况下，调整其概率值：对概率值进行exp运算</p>
<p>同时加入一个参数T （0，1之间），对概率值/T 进行exp运算</p>
<p>则T越小，top概率的token概率被越加放大，输出结果越固定</p>
<p>T越小，输出结果越随机</p>
<p>GPT采取的是先核搜索选择一个token范围，再温控搜索</p>
<h4 id="Mask-掩码层"><a href="#Mask-掩码层" class="headerlink" title="Mask 掩码层"></a>Mask 掩码层</h4><p>1.输入不到最大长度，对输入进行占位符补全</p>
<p>2.transformer操作时，这些占位符不应参与上下文相关的自注意力计算</p>
<p>===》 Mask掩码层</p>
<p>mask 层为每一个 token 位置设置了一个 1，0 取值</p>
<ul>
<li>若设为 1，则对应位置 token 正常进行计算</li>
<li>设为 0 时，对应位置不参与注意力计算。最终的 softmax 概率值为 0。</li>
</ul>
<h3 id="8-监督学习与ChatGPT预训练"><a href="#8-监督学习与ChatGPT预训练" class="headerlink" title="8 监督学习与ChatGPT预训练"></a>8 监督学习与ChatGPT预训练</h3><h4 id="神经网络的训练过程"><a href="#神经网络的训练过程" class="headerlink" title="神经网络的训练过程"></a>神经网络的训练过程</h4><p>监督学习（Supervised Learning） ：是通过给定的输入（ChatGPT 的输入文本）和输出（ChatGPT 的输出文本）数据来学习一个函数，使得对于新的输入数据，可以预测其对应的输出。</p>
<p>将输入数据称为特征，</p>
<p>将输出数据称为标签或目标变量</p>
<p><img src="https://cdn.jsdelivr.net/gh/jdljun/imghosting/img/jianduxuexi.png"></p>
<p><strong>神经网络的输入和输出</strong></p>
<p>两个样本，标注好了其特征和类别</p>
<pre><code>样本 1：体长：0.5 米，身高：0.3 米，食量：0.03 kg；类别：猫
样本 2：体长：1.2 米，身高：0.6 米，食量：0.2 kg；类别：狗
</code></pre>
<p>权重参数：模型需要通过样本进行学习和拟合的</p>
<p>样本 1 的猫类别值：class0=0.5W00 + 0.3W10 + 0.03W20</p>
<p>样本 1 的狗类别值：class1=0.5W01 + 0.3*W11 + 0.03W21</p>
<p>通过比较 class0,class1 这两个结果值的大小，如果猫类别值比较大，说明样本属于猫，反之则属于狗。</p>
<p><strong>神经网络的预测推理</strong></p>
<p>随机初始化一套参数值，用于模型的推理（也叫模型的预测）。</p>
<p>通过初始化的参数值，计算类别值，对类别值计算的结果进行softmax运算，得到预测概率值</p>
<p>若计算结果与标注数据不符合，则需要进一步计算不相符的差距</p>
<p><strong>神经网络的损失函数</strong></p>
<p>损失函数是机器学习中一个重要概念，用于衡量模型预测结果与真实结果之间的差距</p>
<p>常见的损失函数是交叉熵（Cross Entropy）：一个复杂的公式，综合了类别判断是否正确，预测概率值的数据</p>
<p><strong>梯度下降法</strong></p>
<p>随机设定的模型权重，会导致模型对样本的类别判断产生错误。这个错误值的大小即交叉熵损失函数值。</p>
<p>接下来就是让模型能够根据给出的两条样本数据训练模型、更新参数权重，正确地分出每一个样本是猫还是狗。此时，我们就用到了梯度下降法。</p>
<p>定义：沿着函数的负梯度方向不断迭代，直到达到最小值。（求解偏导数）</p>
<p>按照梯度下降法对参数值进行更新，直到求解出足够优质的参数</p>
<h4 id="ChatGPT-的预训练过程"><a href="#ChatGPT-的预训练过程" class="headerlink" title="ChatGPT 的预训练过程"></a>ChatGPT 的预训练过程</h4><p>ChatGPT 的训练流程和上述的猫狗分类没有本质差别</p>
<h3 id="GPT中的few-shot小样本学习"><a href="#GPT中的few-shot小样本学习" class="headerlink" title="GPT中的few shot小样本学习"></a>GPT中的few shot小样本学习</h3><h4 id="纯监督学习"><a href="#纯监督学习" class="headerlink" title="纯监督学习"></a>纯监督学习</h4><p>即依赖【文本，标注信息】的学习方法</p>
<p>缺点</p>
<ul>
<li>极度依赖标注数据集：人工？ 智能？</li>
<li>模型只针对特定任务</li>
<li>模型泛化能力差。</li>
</ul>
<p>（完全随机初始化的参数基础上进行训练）</p>
<h4 id="预训练-微调（Finetune）"><a href="#预训练-微调（Finetune）" class="headerlink" title="预训练+微调（Finetune）"></a>预训练+微调（Finetune）</h4><p>使用大量的未标记数据进行预训练，然后使用少量的标注数据进行微调（finetune），从而适应特定的 NLP 任务</p>
<p>（已经具备一定语言知识和语言能力（经过了预训练）的参数基础上进行训练）</p>
<h4 id="In-context-learning"><a href="#In-context-learning" class="headerlink" title="In-context learning"></a>In-context learning</h4><p><strong>In-context learning 原理</strong></p>
<p>让 GPT 模型在上下文中学习要学的任务和内容</p>
<p>P(output|input,task)</p>
<p>task 就是在原输入基础上，补充指定模型完成的任务。即在对语言模型建模时，模型的输入既要指明文本输入内容，又要指明让模型完成什么任务</p>
<p><strong>In-context learning 的训练</strong></p>
<p>预训练 + 带上下文任务的标准数据微调</p>
<p><strong>zero-shot、one-shot、few-shot 小样本学习</strong></p>
<p><strong>prompt 学习</strong></p>
<p>prompt 就是对任务做自然语言描述，不加 prompt 就是直接给范例。从本质上看，in-context learning 和 prompt 完全就是一回事。只不过描述的语境，具体的操作稍有不同。</p>
<p><strong>In-context learning 的效果评价</strong></p>
<p>只有超大模型（即参数数量非常多）才可以体现出 in-context learning的效果</p>
<p><strong>In-context learning 的局限性</strong></p>
<p>1、in-context learning 无法适应自然语言中灵活的用户指令</p>
<p>2、小样本学习得到的 GPT 模型在做问答任务时，有时会编造虚构事实、产生有误的、带有歧视偏见的回答。</p>

        </div>

        

        
            <div class="article-nav">
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2023/04/12/pu-tong-xin-li-xue/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">普通心理学</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2021</span>
              -
            
            2023&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">jdljun</a>
        </div>
        
            <script async  src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-ChatGPT%E4%B8%8ENLP"><span class="nav-number">1.</span> <span class="nav-text">1 ChatGPT与NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT%E7%9A%84%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F"><span class="nav-number">1.1.</span> <span class="nav-text">ChatGPT的建模方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NLP%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="nav-number">1.2.</span> <span class="nav-text">NLP的发展历史</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NLP%E6%8A%80%E6%9C%AF%E7%9A%84%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C"><span class="nav-number">1.3.</span> <span class="nav-text">NLP技术的发展脉络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-Transformer"><span class="nav-number">1.4.</span> <span class="nav-text">ChatGPT的神经网络结构 Transformer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%8EGPT1-0%E5%88%B0ChatGPT"><span class="nav-number">2.</span> <span class="nav-text">2 从GPT1.0到ChatGPT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT%E5%88%9D%E4%BB%A3"><span class="nav-number">2.1.</span> <span class="nav-text">GPT初代</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-2"><span class="nav-number">2.2.</span> <span class="nav-text">GPT-2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-3"><span class="nav-number">2.3.</span> <span class="nav-text">GPT-3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT"><span class="nav-number">2.4.</span> <span class="nav-text">ChatGPT</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-ChatGPT%E6%98%AF%E4%B8%80%E4%B8%AA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">3 ChatGPT是一个语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="nav-number">3.1.</span> <span class="nav-text">ChatGPT语言模型的数学建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.2.</span> <span class="nav-text">ChatGPT的语言模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-ChatGPT%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%96%87%E5%AD%97%E8%BE%93%E5%85%A5"><span class="nav-number">4.</span> <span class="nav-text">4 ChatGPT如何处理文字输入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tokenizer"><span class="nav-number">4.1.</span> <span class="nav-text">Tokenizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5-Embedding"><span class="nav-number">4.2.</span> <span class="nav-text">词嵌入(Embedding)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Attention%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">5.</span> <span class="nav-text">5 Attention注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BB%BA%E6%A8%A1"><span class="nav-number">5.1.</span> <span class="nav-text">注意力机制的建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="nav-number">5.2.</span> <span class="nav-text">注意力机制的好处</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-transformer%E7%BB%93%E6%9E%84"><span class="nav-number">6.</span> <span class="nav-text">6 transformer结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#transFormer%E7%BB%93%E6%9E%84"><span class="nav-number">6.1.</span> <span class="nav-text">transFormer结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Encoder-Decoder"><span class="nav-number">7.</span> <span class="nav-text">7 Encoder-Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT-%E4%B8%AD%E7%9A%84%E7%BC%96%E8%A7%A3%E7%A0%81%E6%9E%B6%E6%9E%84"><span class="nav-number">7.1.</span> <span class="nav-text">GPT 中的编解码架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%BE%97%E5%88%B0%E8%BE%93%E5%87%BA-Token"><span class="nav-number">7.2.</span> <span class="nav-text">解码得到输出 Token</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mask-%E6%8E%A9%E7%A0%81%E5%B1%82"><span class="nav-number">7.3.</span> <span class="nav-text">Mask 掩码层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8EChatGPT%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">8.</span> <span class="nav-text">8 监督学习与ChatGPT预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">8.1.</span> <span class="nav-text">神经网络的训练过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ChatGPT-%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">8.2.</span> <span class="nav-text">ChatGPT 的预训练过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT%E4%B8%AD%E7%9A%84few-shot%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="nav-number">9.</span> <span class="nav-text">GPT中的few shot小样本学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%AF%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">9.1.</span> <span class="nav-text">纯监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83-%E5%BE%AE%E8%B0%83%EF%BC%88Finetune%EF%BC%89"><span class="nav-number">9.2.</span> <span class="nav-text">预训练+微调（Finetune）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#In-context-learning"><span class="nav-number">9.3.</span> <span class="nav-text">In-context learning</span></a></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
